chart:
  name: data-pipeline
  version: 1.1.0
  description: Data Pipeline Chart
files:
  - path: Chart.yaml
    template: true
    content: |
      apiVersion: v2
      name: {{ .Values.chart.name }}
      version: {{ .Values.chart.version }}
      description: {{ .Values.chart.description }}
      type: application
      files:
      - src/**
      dependencies:
        - name: dognauts-spark-app-basis
          alias: sparkAppBasis
          version: {{ .Values.chart.dependency.version }}
          repository: "{{ .Values.chart.dependency.repository }}"
          condition: spark-app.enabled
      app.dognauts:
        subjectArea: {{ .Values.subjectArea }}
        sourceMetafileRepo: {{ .Values.sourceMetafileRepo }}
        sourceMetafileBranch: {{ .Values.sourceMetafileBranch }}
        sourceMetafileName: {{ .Values.sourceMetafileName }}
      annotations:
        app.dognauts/platform-generated: true
        app.dognauts/subjectArea: {{ .Values.subjectArea }}
        {{- if .Values.code.model.name }}
        dognauts.batch/model.name: {{ .Values.code.model.name }}
        {{- end }}
        {{- if .Values.code.model.alias }}
        dognauts.batch/model.alias: {{ .Values.code.model.alias}}
        {{- end }}

  - path: templates/configmap-code.yaml
    content: |
      {{- /* Сбор файлов из директории src */}}
      {{- $files := dict }}
      {{- range $path, $_ := .Files.Glob "src/**" }}
        {{- $key := $path | trimPrefix "src/" | replace "/" "_" }}
        {{- $content := $.Files.Get $path }}
        {{- $files = set $files $key $content }}
      {{- end }}
      
      {{- $mergedValues := merge (deepCopy $.Values) (dict "global" $.Values.global) }}
  
      {{- /* Вызов шаблона из базового чарта */}}
      {{- include "spark-app-base.configmap-code" (dict 
        "Release" .Release 
        "Values" $mergedValues 
        "files" $files
      ) }}

  - path: templates/configmap-requirements.yaml
    content: |
      {{- /* Получение содержимого requirements.txt и conda.yaml */}}
      {{- $requirements := .Files.Get "src/requirements.txt" }}
      {{- $conda := .Files.Get "src/conda.yaml" }}
      
      {{- $mergedValues := merge (deepCopy $.Values) (dict "global" $.Values.global) }}

      {{- /* Вызов шаблона с объединением параметров в один dict */}}
      {{- include "spark-app-base.configmap-requirements" (dict 
        "Release" .Release 
        "Values" $mergedValues 
        "requirements" $requirements 
        "conda" $conda
      ) }}

  - path: templates/batch-secret.yaml
    content: |
      apiVersion: v1
      kind: Secret
      metadata:
        name: {{ .Release.Name }}-secret
        namespace: {{ .Release.Namespace }}
        {{- with .Values.annotations }}
        annotations:
        {{- toYaml . | nindent 6 }}
        {{- end }}
      data:
        AWS_ACCESS_KEY_ID: {{ .Values.sparkAppBasis.spark.s3AccessKey | b64enc }}
        AWS_SECRET_ACCESS_KEY: {{ .Values.sparkAppBasis.spark.s3SecretKey | b64enc }}
        AWS_ENDPOINT_URL: {{ .Values.sparkAppBasis.spark.awsEndpointUrl | b64enc }}
        BATCH_S3_ENDPOINT_URL: {{ .Values.sparkAppBasis.spark.batchS3EndpointUrl | b64enc }}
        MLFLOW_INTERNAL_URL: {{ .Values.sparkAppBasis.spark.mlflowInternalUrl | b64enc }}

  - path: src/conda.yaml
    content: |
      channels:
      - conda-forge
      dependencies:
      - python=3.8.10
      - pip<=23.2.1
      - pip:
        - mlflow==2.17.2
        - cloudpickle==3.1.1
        - numpy==1.24.4
        - scikit-learn==1.3.2
        - scipy==1.10.1
      name: mlflow-env      

  - path: src/main.py
    template: true
    content: |
      from pyspark.sql import SparkSession
      from pyspark.sql.functions import struct
      from datetime import datetime
      import mlflow.pyfunc
      import os

      def main():
        spark = SparkSession.builder.appName("{{ .Values.appName }}").getOrCreate()
        
        s3_access_key = os.getenv("AWS_ACCESS_KEY_ID")
        s3_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
        s3_host_port = os.getenv("BATCH_S3_ENDPOINT_URL")
        mlflow_host_port = os.getenv("MLFLOW_INTERNAL_URL")
        s3_input_path = os.getenv("S3_INPUT_PATH")
        s3_output_path = os.getenv("S3_OUTPUT_PATH")
                
        if not s3_access_key or not s3_secret_key:
          raise ValueError("AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY must be set in the environment variables")
          
        if not s3_host_port:
          raise ValueError("BATCH_S3_ENDPOINT_URL must be set in the environment variables")
        
        if not mlflow_host_port:
          raise ValueError("MLFLOW_INTERNAL_URL must be set in the environment variables")
          
        if not s3_input_path:
          raise ValueError("s3_input_path must be properly configured")
          
        if not s3_output_path:
          raise ValueError("s3_output_path must be properly configured")

        spark.sparkContext._jsc \
            .hadoopConfiguration().set("fs.s3a.access.key", s3_access_key)
        spark.sparkContext._jsc \
            .hadoopConfiguration().set("fs.s3a.secret.key", s3_secret_key)
        spark.sparkContext._jsc \
            .hadoopConfiguration().set("fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        spark.sparkContext._jsc \
            .hadoopConfiguration().set("fs.s3a.aws.credentials.provider",
                                   "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider")
        spark.sparkContext._jsc \
            .hadoopConfiguration().set("fs.s3a.path.style.access", "true")
        spark.sparkContext._jsc \
            .hadoopConfiguration().set("fs.s3a.endpoint", s3_host_port)

        {{- if .Values.code.model.useVersion }}
        model_uri = "models:/{{ .Values.code.model.name }}/{{ .Values.code.model.version }}"
        {{- else }}
        model_uri = "models:/{{ .Values.code.model.name }}@{{ .Values.code.model.alias }}"
        {{- end }}
        
        mlflow.set_tracking_uri(mlflow_host_port)

        model_udf = mlflow.pyfunc.spark_udf(spark, model_uri)

        input_df = spark.read.parquet(s3_input_path)

        output_df = input_df.withColumn("{{ .Values.code.model.predictionColumnName }}", model_udf(struct(*input_df.columns)))

        output_df.write.mode("append").parquet(s3_output_path)

        output_df.show()

        spark.stop()

      if __name__ == "__main__":
        main()

  - path: src/requirements.txt
    template: true
    content: |
      #pyspark==3.5.5
      #mlflow==2.9.2
      #pandas==2.1.3
      #pyarrow==14.0.1
      #feast==0.45.0

  - path: values.yaml
    template: true
    content: |
      subjectArea: {{ .Values.subjectArea }}
      sparkAppBasis:
        subjectArea: {{ .Values.subjectArea }}
        global:
          parentChart:
            name: {{ .Values.chart.name }}   
            version: {{ .Values.chart.version }}
            subjectArea: {{ .Values.subjectArea }}
        spark:
          mainApplicationFilename: "{{ .Values.mainAppFile }}"
          ui:
            port: {{ .Values.deploy.spark.ui.port }}
          annotations:
            {{- if .Values.code.model.name }}
            dognauts.batch/model.name: {{ .Values.code.model.name }}
            {{- end }}
            {{- if .Values.code.model.alias }}
            dognauts.batch/model.alias: {{ .Values.code.model.alias}}
            {{- end }}

  - path: values-env.yaml
    outputFilename: values-{{ .Values.environment }}.yaml
    template: true
    content: |
      annotations:
        vault.security.banzaicloud.io/registry-skip-verify: 'true'
        vault.security.banzaicloud.io/vault-addr: https://vault.vault-infra:8200
        vault.security.banzaicloud.io/vault-role: default
        vault.security.banzaicloud.io/vault-skip-verify: 'true'

      sparkAppBasis:
        clusterDomain: {{ .Values.clusterDomain }}
        k8sPlatformType: {{ .Values.k8sPlatformType }}
        kubeflow:
          sparkOperator:
            serviceAccount: {{ .Values.deploy.spark.serviceAccount }}
            namespace: {{ .Values.deploy.spark.namespace }}    
    
        spark:
          image:
            repository: {{ .Values.deploy.spark.image.repository }}
            tag: {{ .Values.deploy.spark.image.tag }}
            pullPolicy: {{ .Values.deploy.spark.image.pullPolicy }}
          s3SecretName: {{ .Values.deploy.spark.s3SecretName }}-secret
          s3AccessKey: {{ .Values.deploy.spark.s3AccessKey }}
          s3SecretKey: {{ .Values.deploy.spark.s3SecretKey }}
          awsEndpointUrl: {{ .Values.deploy.spark.awsEndpointUrl }}
          batchS3EndpointUrl: {{ .Values.deploy.spark.batchS3EndpointUrl }}
          mlflowInternalUrl: {{ .Values.deploy.spark.mlflowInternalUrl }}
          dognauts:
            enabled: true
          historyServerProjectURL: "{{ .Values.deploy.spark.implicit.historyServerProjectURL }}"
          eventLogEnabled: "{{ .Values.deploy.spark.eventLogEnabled }}"
          eventLogDir: "{{ .Values.deploy.spark.eventLogDir }}"
          schedule: "{{ .Values.deploy.spark.schedule }}"
          concurrencyPolicy: {{ .Values.deploy.spark.concurrencyPolicy }}
          successfulRunHistoryLimit: {{ .Values.deploy.spark.successfulRunHistoryLimit }}
          failedRunHistoryLimit: {{ .Values.deploy.spark.failedRunHistoryLimit }}
          dynamicAllocation:
            enabled: {{ .Values.deploy.spark.dynamicAllocation.enabled }}
            initialExecutors: {{ .Values.deploy.spark.dynamicAllocation.initialExecutors }}
            minExecutors: {{ .Values.deploy.spark.dynamicAllocation.minExecutors }}
            maxExecutors: {{ .Values.deploy.spark.dynamicAllocation.maxExecutors }}
          driver:
            javaOptions:
              enabled: {{ .Values.deploy.spark.driver.javaOptions.enabled }}
              values: {{ .Values.deploy.spark.driver.javaOptions.value }}
            gpu:
              enabled: {{ .Values.deploy.spark.driver.gpu.enabled }}
              name: {{ .Values.deploy.spark.driver.gpu.name }}
              quantity: {{ .Values.deploy.spark.driver.gpu.quantity }}
            cores: {{ .Values.deploy.spark.driver.cores }}
            coreLimit: {{ .Values.deploy.spark.driver.coreLimit }}
            coreRequest: {{ .Values.deploy.spark.driver.coreRequest }}
            memory: "{{ .Values.deploy.spark.driver.memory }}"
            useEnv: {{ .Values.deploy.spark.driver.useEnv }}
            {{- if .Values.deploy.spark.driver.useEnv }}
            env:
            {{- range .Values.deploy.spark.driver.env }}
              - name: {{ .name }}
                {{- if .value }}
                value: {{ .value }}
                {{- end }}
            {{- end }}
              - name: S3_INPUT_PATH
                value: {{ .Values.deploy.spark.s3inputPath }}
              - name: S3_OUTPUT_PATH
                value: {{ .Values.deploy.spark.s3outputPath }}
            {{- end }}
            initContainers:
              alpineImage: {{ .Values.deploy.spark.driver.initContainers.alpineImage }}
          executor:
            javaOptions:
                enabled: {{ .Values.deploy.spark.executor.javaOptions.enabled }}
                values: {{ .Values.deploy.spark.executor.javaOptions.value }}
            gpu:
              enabled: {{ .Values.deploy.spark.executor.gpu.enabled }}
              name: {{ .Values.deploy.spark.executor.gpu.name }}
              quantity: {{ .Values.deploy.spark.executor.gpu.quantity }}
            instances: {{ .Values.deploy.spark.executor.instances }}
            cores: {{ .Values.deploy.spark.executor.cores }}
            coreLimit: {{ .Values.deploy.spark.executor.coreLimit }}
            coreRequest: {{ .Values.deploy.spark.executor.coreRequest }}
            memory: "{{ .Values.deploy.spark.executor.memory }}"
            useEnv: {{ .Values.deploy.spark.executor.useEnv }}
            {{- if .Values.deploy.spark.executor.useEnv }}
            env:
            {{- range .Values.deploy.spark.executor.env }}
              - name: {{ .name }}
                {{- if .value }}
                value: {{ .value }}
                {{- end }}
            {{- end }}
              - name: S3_INPUT_PATH
                value: {{ .Values.deploy.spark.s3inputPath }}
              - name: S3_OUTPUT_PATH
                value: {{ .Values.deploy.spark.s3outputPath }}
            {{- end }}
          restartPolicy: 
            enabled: {{ .Values.deploy.spark.restartPolicy.enabled }}
            type: {{ .Values.deploy.spark.restartPolicy.type }}
            onFailureRetries: {{ .Values.deploy.spark.restartPolicy.onFailureRetries }}
            onFailureRetryInterval: {{ .Values.deploy.spark.restartPolicy.onFailureRetries }}
            onSubmissionFailureRetries: {{ .Values.deploy.spark.restartPolicy.onSubmissionFailureRetries }}
            onSubmissionFailureRetryInterval: {{ .Values.deploy.spark.restartPolicy.onSubmissionFailureRetryInterval }}


valuesSchema:
  chart.description:
    type: string
    description: A single-sentence description of this project
    required: true
  subjectArea:
    type: string
    description: A tag to classify a subject area of the target chart.
    required: true
    default: batch-model-inference
  sourceMetafileName:
    type: string
    description: Source metafile name
    required: true
    default: no_data
  sourceMetafileRepo:
    type: string
    description: Source metafile name
    required: true
    default: no_data
  sourceMetafileBranch:
    type: string
    description: Source metafile name
    required: true
    default: no_data
  mainAppFile:
    type: string
    description: Main application filename
    required: true
    default: main.py
  appName:
    type: string
    description: Main application display name
    required: true
  clusterDomain:
    type: string
    description: The base DNS domain for Kubernetes cluster services.
    requred: true
    default: cluster.local
  k8sPlatformType:
    type: string
    description: The k8s platform type (Deckhouse, General, Generic, Default)
    requred: true
    default: Default
  chart.name:
    type: string
    description: Name of the destination Chart
    required: true
  chart.version:
    type: string
    description: Version of the destination Chart
    required: true
  chart.dependency.version:
    type: string
    description: Version of the parent Chart
    required: true
  chart.dependency.repository:
    type: string
    description: A link to the related parent Chart repository
    required: true
  code.model.inputdata:
    type: string
    description: A path to the input data
    required: true
  code.model.outputdata:
    type: string
    description: A path to store predictions
    required: true
  code.model.predictionColumnName:
    type: string
    description: A name for result column, that contains a prediction
    required: true
  predictionColumnName:
    type: string
    description: A name for result column, that contains a prediction
    required: true
  code.model.version:
    type: string
    description: A model version from MLFlow
    required: false
  code.model.name:
    type: string
    description: A model name from MLFlow
    required: true
  code.model.alias:
    type: string
    description: A model alias from MLFlow
    required: true
  code.model.useVersion:
    type: boolean
    description: A flag whether to use model version instead of model alias
    required: false
  code.feast.repo:
    type: string
    description: A path to a Feast Repository
    required: true
  code.feast.edfSql:
    type: string
    description: SQL-query for entity dataframe
    required: true
  code.feast.featureService:
    type: string
    description: A name of a Feast feature service
    required: true
  deploy.spark.image.repository:
    type: string
    description: A path to spark image
    required: true
    default: not-provided
  deploy.spark.image.tag:
    type: string
    description: A tag of the spark image
    required: true
    default: not-provided
  deploy.spark.image.pullPolicy:
    type: string
    description: Spark image pull policy
    required: true
    default: not-provided
  deploy.spark.serviceAccount:
    type: string
    description: A name of a service account to run Apache Spark jobs
    required: true
  deploy.spark.ui.port:
    type: integer
    description: A port for Spark UI
    required: true
    default: 4045
  deploy.spark.eventLogEnabled:
    type: string
    description: A port for Spark UI
    required: true
    default: "true"
  deploy.spark.eventLogDir:
    type: string
    description: The path to write logs
    required: true
    default: "file:/data"
  deploy.spark.namespace:
    type: string
    description: A namespace to run Apache Spark jobs
    required: true
  deploy.spark.driver.javaOptions.enabled:
    type: boolean
    description: The Flag whether to put extra java options to driver node
    required: false
    default: false
  deploy.spark.driver.useEnv:
    type: boolean
    description: The Flag whether to use specific env variables in spark driver
    required: false
    default: false
  deploy.spark.driver.javaOptions.value:
    type: string
    description: Extra java options for driver node
    required: true
  deploy.spark.driver.cores:
    type: float
    description: A number of driver cores
    required: true
  deploy.spark.driver.coreLimit:
    type: string
    description: A core limit for Spark driver
    required: true
  deploy.spark.driver.coreRequest:
    type: string
    description: A core request for Spark driver
    required: true
  deploy.spark.driver.memory:
    type: string
    description: A number of driver memory
    required: true
  deploy.spark.driver.gpu.enabled:
    type: boolean
    description: Flag whether GPU should be enabled for driver
    required: false
  deploy.spark.driver.gpu.name:
    type: string
    description: A GPU name for driver
    required: true
  deploy.spark.driver.gpu.quantity:
    type: integer
    description: A GPU quantity for driver
    required: true
  deploy.spark.driver.env:
    type: list
    description: A list of key-value pairs Env Name -> Value
    required: true
  deploy.spark.driver.initContainers.alpineImage:
    type: string
    description: The path to alpine init container image
    required: true
  deploy.spark.executor.memory:
    type: string
    description: A number of executor memory
    required: true
  deploy.spark.executor.cores:
    type: float
    description: A number of executor cores
    required: true
  deploy.spark.executor.coreLimit:
    type: string
    description: A core limit for Spark executor
    required: true
  deploy.spark.executor.coreRequest:
    type: string
    description: A core request for Spark driver
    required: true
  deploy.spark.executor.instances:
    type: integer
    description: A number of executor cores
    required: true
  deploy.spark.executor.gpu.enabled:
    type: boolean
    description: Flag whether GPU should be enabled for executor
    required: false
  deploy.spark.executor.gpu.quantity:
    type: boolean
    description: A GPU quantity for executor
    required: false
  deploy.spark.executor.gpu.name:
    type: boolean
    description: A GPU name for executor
    required: false
  deploy.spark.executor.javaOptions.enabled:
    type: boolean
    description: The Flag whether to put extra java options to executor nodes
    required: false
  deploy.spark.executor.javaOptions.value:
    type: string
    description: Extra java options for executor nodes
    required: true
  deploy.spark.executor.useEnv:
    type: boolean
    description: A Flag whether to use env variables for executor container
    required: true
  deploy.spark.executor.env:
    type: list
    description: A list of key-value pairs Env Name -> Value
    required: true
  deploy.spark.schedule:
    type: string
    description: A schedule for ScheduledSparkApplication
    required: true
  deploy.spark.concurrencyPolicy:
    type: string
    description: Spark application concurrency Allow Forbid and Replace
    required: true
  deploy.spark.successfulRunHistoryLimit:
    type: integer
    description:
    required: true
  deploy.spark.failedRunHistoryLimit:
    type: integer
    description:
    required: true
  deploy.spark.dynamicAllocation.enabled:
    type: boolean
    description: Enables dynamic allocation for cluster resource management
    required: false
  deploy.spark.dynamicAllocation.initialExecutors:
    type: integer
    description: The quantity of initial executors
    required: true
  deploy.spark.dynamicAllocation.minExecutors:
    type: integer
    description: The minimum quantity of executors
    required: true
  deploy.spark.dynamicAllocation.maxExecutors:
    type: integer
    description: The maximum quantity of executors
    required: true
  deploy.spark.restartPolicy.enabled:
    type: boolean
    description: The flag whether to use restart policy for Spark jobs
    required: false
    default: false
  deploy.spark.restartPolicy.type:
    type: string
    description: The type of restart policy. Never, Always or OnFailure
    required: true
  deploy.spark.restartPolicy.onFailureRetries:
    type: integer
    description: Setting limits on number of retries
    required: true
  deploy.spark.restartPolicy.onSubmissionFailureRetries:
    type: integer
    description: Setting limits on number of retries
    required: true
  deploy.spark.restartPolicy.onSubmissionFailureRetryInterval:
    type: integer
    description: Setting limits on number of retries
    required: true
  deploy.spark.implicit.historyServerProjectURL:
    type: string
    description: A path to the root S3 project bucket
    required: true
  deploy.spark.s3AccessKey:
    type: string
    description: A path to S3 s3AccessKey
    required: true
  deploy.spark.s3SecretKey:
    type: string
    description: A path to the S3 SecretKey
    required: true
  deploy.spark.s3SecretName:
    type: string
    description: A name of the S3 Secret
    required: true
  deploy.spark.awsEndpointUrl:
    type: string
    description: A path to awsEndpointUrl
    required: true
  deploy.spark.batchS3EndpointUrl:
    type: string
    description: A path to the batchS3EndpointUrl
    required: true
  deploy.spark.mlflowInternalUrl:
    type: string
    description: A path to the mlflowInternalUrl
    required: true
  deploy.spark.s3inputPath:
    type: string
    description: A path to the input of datasets
    required: true
  deploy.spark.s3outputPath:
    type: string
    description: A path to the output of datasets
    required: true
  sparkAppBasis.spark.s3AccessKey:
    type: string
    description: A path to S3 s3AccessKey
    required: true
  sparkAppBasis.spark.s3SecretKey:
    type: string
    description: A path to the S3 SecretKey
    required: true
  sparkAppBasis.spark.s3SecretName:
    type: string
    description: A name of the S3 Secret
    required: true
  sparkAppBasis.spark.awsEndpointUrl:
    type: string
    description: A path to awsEndpointUrl
    required: true
  sparkAppBasis.spark.batchS3EndpointUrl:
    type: string
    description: A path to the batchS3EndpointUrl
    required: true
  sparkAppBasis.spark.mlflowInternalUrl:
    type: string
    description: A path to the mlflowInternalUrl
    required: true